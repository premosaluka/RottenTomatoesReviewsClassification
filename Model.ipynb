{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found a spreadsheet containing 480.000 reviews from RottenTomato. My goal here is to create a model which will read the review and predict if the review is positive (fresh) or negative (rotten)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from csv file using pandas\n",
    "import pandas as pd\n",
    "data=pd.read_csv(\"rt_reviews.csv\",engine='python', encoding='ISO-8859-1', error_bad_lines=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Freshness</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fresh</td>\n",
       "      <td>Manakamana doesn't answer any questions, yet ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fresh</td>\n",
       "      <td>Wilfully offensive and powered by a chest-thu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rotten</td>\n",
       "      <td>It would be difficult to imagine material mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rotten</td>\n",
       "      <td>Despite the gusto its star brings to the role...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rotten</td>\n",
       "      <td>If there was a good idea at the core of this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479995</th>\n",
       "      <td>rotten</td>\n",
       "      <td>Zemeckis seems unable to admit that the motio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479996</th>\n",
       "      <td>fresh</td>\n",
       "      <td>Movies like The Kids Are All Right -- beautif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479997</th>\n",
       "      <td>rotten</td>\n",
       "      <td>Film-savvy audiences soon will catch onto Win...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479998</th>\n",
       "      <td>fresh</td>\n",
       "      <td>An odd yet enjoyable film.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479999</th>\n",
       "      <td>fresh</td>\n",
       "      <td>No other animation studio, even our beloved P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>480000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Freshness                                             Review\n",
       "0          fresh   Manakamana doesn't answer any questions, yet ...\n",
       "1          fresh   Wilfully offensive and powered by a chest-thu...\n",
       "2         rotten   It would be difficult to imagine material mor...\n",
       "3         rotten   Despite the gusto its star brings to the role...\n",
       "4         rotten   If there was a good idea at the core of this ...\n",
       "...          ...                                                ...\n",
       "479995    rotten   Zemeckis seems unable to admit that the motio...\n",
       "479996     fresh   Movies like The Kids Are All Right -- beautif...\n",
       "479997    rotten   Film-savvy audiences soon will catch onto Win...\n",
       "479998     fresh                        An odd yet enjoyable film. \n",
       "479999     fresh   No other animation studio, even our beloved P...\n",
       "\n",
       "[480000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fresh', 'rotten'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.unique(data.Freshness)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is pretty much clean. All reviews seem to have a valid label. Some reviews might have special characters or out-of-vocabulary words. Those words will be replaced later with built-in tensorflow functions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function that finds the longest reviews (most words)\n",
    "def longest_sentece(sentences):\n",
    "    max_l = 0    \n",
    "    for s in sentences:\n",
    "        length = len(s)\n",
    "        if max_l < length:\n",
    "            max_l = length\n",
    "    return max_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification problems are usually tackled with word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First we need to tokenize all words from all our reviews. Each unique word will be asigned unique integer\n",
    "## We can use Tokenizer function from tensorflow\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   39    60    69     3    52   342    27     2   919     5    14    16\n",
      "    17    77  2400     8    18 29790  2926     5 11733   362  1025  5201\n",
      "   126  7114     4     3   824  2927 40291   139     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "# Split our data in sentences and labels \n",
    "sentences,labels = data.Review, data.Freshness\n",
    "\n",
    "from sklearn import preprocessing\n",
    "## We need to label positive reviews as 0 and negative reviews as 1\n",
    "## We can use LabelEncoder function from sklearn\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "labels = le.transform(labels)\n",
    "\n",
    "# Data cleaning and tokenization\n",
    "## We want to remove all special characters, turn to lowercase, and replace all out-of-vocabulary words with <OOV>\n",
    "tokenizer = Tokenizer(\n",
    "    num_words=None,\n",
    "    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "    lower=True, split=' ', char_level=False, oov_token=\"<OOV>\",\n",
    "    document_count=0)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "# Turning text to int sequences\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "max_len = longest_sentece(sequences)\n",
    "\n",
    "# Pad all sequences to have the same length (add zeros to the end)\n",
    "padded = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "print(padded[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Longest sequence/sentence\n",
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Longest review has 55 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are some pre-trained model that can save us time with word embedding.\n",
    "# We can use gensim to access them\n",
    "## I picked 'glove-twitter-25' model, which transforms every word into a vector of dimension 25\n",
    "import gensim.downloader as api\n",
    "\n",
    "word2vec = api.load('glove-wiki-gigaword-50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dictionary of our tokenized words\n",
    "word_index = tokenizer.word_index\n",
    "## Calculate mean and std in imported word2vec matrix/model\n",
    "## These values will be used to generate random vectors for words, which are not included in pre-trained model\n",
    "emb_mean = word2vec.vectors.mean()\n",
    "emb_std = word2vec.vectors.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also check how many of our words are included in downloaded pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that finds number of included words in pretrained embedding model\n",
    "def check_words(word_index, word_matrix_model):\n",
    "    count = 0\n",
    "    for word in word_index:\n",
    "        if word in word_matrix_model:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64873"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_words(word_index, word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102046"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## How many different words we have in our reviews\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only roughly half of our words are in pretrained model. In this case it is probably better to just train an embedding layer ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split data in learning in validation set\n",
    "# Very simple split: 2/3 of data is used for training and 1/3 for validation\n",
    "\n",
    "x_test,y_test = padded[:len(padded)//3],labels[:len(labels)//3]\n",
    "x_train,y_train = padded[len(padded)//3:],labels[len(labels)//3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport numpy as np\\ndef pretrained_embedding_matrix(word_to_vec_map, word_to_index, emb_mean, emb_std):\\n    \\n    np.random.seed(1)\\n    \\n    # adding 1 to fit Keras embedding (requirement) (=102046 + 1)\\n    vocab_size = len(word_to_index) + 1\\n    # define dimensionality of your pre-trained word vectors (= 300)\\n    emb_dim = word_to_vec_map.get_vector(\\'news\\').shape[0]\\n    \\n    # initialize the matrix with generic normal distribution values\\n    embed_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, emb_dim))\\n    \\n    # Set each row \"idx\" of the embedding matrix to be \\n    # the word vector representation of the idx\\'th word of the vocabulary\\n    for word, idx in word_to_index.items():\\n        if word in word_to_vec_map:\\n            embed_matrix[idx] = word_to_vec_map.get_vector(word)\\n            \\n    return embed_matrix\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We dont need this function since we are training our embedding layer ourselves\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "def pretrained_embedding_matrix(word_to_vec_map, word_to_index, emb_mean, emb_std):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    # adding 1 to fit Keras embedding (requirement) (=102046 + 1)\n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    # define dimensionality of your pre-trained word vectors (= 300)\n",
    "    emb_dim = word_to_vec_map.get_vector('news').shape[0]\n",
    "    \n",
    "    # initialize the matrix with generic normal distribution values\n",
    "    embed_matrix = np.random.normal(emb_mean, emb_std, (vocab_size, emb_dim))\n",
    "    \n",
    "    # Set each row \"idx\" of the embedding matrix to be \n",
    "    # the word vector representation of the idx'th word of the vocabulary\n",
    "    for word, idx in word_to_index.items():\n",
    "        if word in word_to_vec_map:\n",
    "            embed_matrix[idx] = word_to_vec_map.get_vector(word)\n",
    "            \n",
    "    return embed_matrix\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to construct our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tcn import TCN, tcn_full_summary\n",
    "from tensorflow.keras.layers import Conv1D, Embedding, Dense, Dropout, SpatialDropout1D, Input\n",
    "from tensorflow.keras.layers import concatenate, GlobalAveragePooling1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 55)]              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 55, 25)            2551175   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 55, 16)            3216      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 27, 16)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 432)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                6928      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 2,561,336\n",
      "Trainable params: 2,561,336\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, voc_size, max_length, output_dim=25):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.max_length = max_length\n",
    "        self.embedding = Embedding(input_dim=voc_size, output_dim=output_dim, input_length=max_length, \n",
    "                                   trainable = True)\n",
    "        \n",
    "        self.conv1d = Conv1D(16, 8, activation='relu',padding='same')\n",
    "        self.max_pool = MaxPooling1D()\n",
    "        ## Flatten the matrix to a vector\n",
    "        self.flat = Flatten()\n",
    "        self.dense1 = Dense(16, activation=\"relu\")\n",
    "        # Output needs to be between 0 and 1 therefore final activation with sigmoid function\n",
    "        self.dense2 = Dense(1, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.conv1d(x)\n",
    "        x = self.max_pool(x)\n",
    "        x = self.flat(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "    \n",
    "    def model(self):\n",
    "        x = Input(shape=(self.max_length))\n",
    "        return Model(inputs=[x], outputs=self.call(x))\n",
    "\n",
    "    \n",
    "    \n",
    "model = MyModel(voc_size=len(word_index)+1, max_length=55, output_dim=25 )\n",
    "model.model().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile( loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train model on training data and validate on validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "32000/32000 [==============================] - 788s 25ms/step - loss: 0.4576 - accuracy: 0.7752 - val_loss: 0.3487 - val_accuracy: 0.8467\n",
      "Epoch 2/3\n",
      "32000/32000 [==============================] - 784s 24ms/step - loss: 0.2619 - accuracy: 0.8929 - val_loss: 0.3203 - val_accuracy: 0.8666\n",
      "Epoch 3/3\n",
      "32000/32000 [==============================] - 780s 24ms/step - loss: 0.1721 - accuracy: 0.9339 - val_loss: 0.3255 - val_accuracy: 0.8710\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c24880c820>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train, batch_size = 10, epochs = 3, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model scored accuracy of 93,4% on training dataset and 87.0% on validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at some examples of missclassifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## first 10 sentences from validation set\n",
    "model.predict(x_test[0:10],).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 1, 1, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels of fist 10 sentences in validation set\n",
    "y_test[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In first 10 examples we can find 2 missclassifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Everyone in \"The Comedian\" deserves a better movie than \"The Comedian.\"'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missclassification\n",
    "# This review is hard for model to understand\n",
    "sentences[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Slight, contained, but ineffably soulful.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missclassification\n",
    "sentences[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try to classify a custom sentence/review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 24,   2, 349,  11,   9,  69,  83, 135,   4, 490,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_example = ['Not the worst but it was very long and boring']\n",
    "## First tokenize it \n",
    "test_sequence = tokenizer.texts_to_sequences(test_example)\n",
    "## Pad it to length = 55\n",
    "test_sequence= pad_sequences(test_sequence, maxlen=max_len, padding='post', truncating='post')\n",
    "test_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test_sequence[0:1],).round()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model predicts that our test_example is a negative review which is correct."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
